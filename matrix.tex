\twocolumn[
\begin{center}
\textbf{\Large Matrix Algebra}
\end{center}
\vspace{4pt}]
%----------------------------------------------
\textbf{Commutativity:} $AM = MA \Rightarrow$ matrices commute

\textbf{Nilpotent (class $p$):} $A^p = 0$ but $A^{p-1} \neq 0$

\textbf{Involutory:} $A^2 = I$, hence $A^{-1} = A$

\textbf{Orthogonal:} $A^T A = I$ (columns are orthonormal)

Orthonormal means: (1) unit vectors: $a^2 + c^2 = 1$, (2) perpendicular: $ab + cd = 0$

\textbf{Unitary:} $A^* A = I$ where $A^* = \overline{A}^T$ (conjugate transpose)

\textbf{Symmetric:} $A^T = A$ \quad \textbf{Skew-Symmetric:} $A^T = -A$

\textbf{Normal:} $AA^T = A^T A$ (real), or $A^* A = AA^*$ (complex)

\textbf{Orthogonal Vectors:} Set is orthogonal if every dot 
product pair $= 0$

\hrulefill

\vspace{4pt}
\textbf{\large Inverse of a Matrix}

$AA^{-1} = I$. Finding inverse: $A^{-1} = \dfrac{\text{adj}(A)}{\det(A)} = \dfrac{[C_{ij}]^T}{\det(A)}$

\textbf{Cofactor:} $C_{ij} = (-1)^{i+j} \det(A_{ij})$ where $A_{ij}$ is $A$ with row $i$, col $j$ removed.

\textbf{2$\times$2 Inverse Formula:}
$A^{-1} = \dfrac{1}{\det A}\bigl((\text{tr}\,A)I - A\bigr)$

where trace $= $ sum of diagonal elements.

\textbf{Row Reduction Method:} Augment $[A \mid I]$, row reduce to $[I \mid A^{-1}]$.

Example: $\left[\begin{array}{cc|cc} 2 & 1 & 1 & 0 \\ 5 & 3 & 0 & 1 \end{array}\right] \rightarrow \left[\begin{array}{cc|cc} 1 & 0 & * & * \\ 0 & 1 & * & * \end{array}\right]$

\vspace{4pt}
\textbf{\large Determinants}

\textbf{If row/column is multiple of another:} $\det = 0$

\textbf{Properties:} $\det(AB) = \det A \cdot \det B$, \quad $\det(A^T) = \det A$

$\det(kA) = k^n \det A$ for $n \times n$, \quad $\det(A^{-1}) = 1/\det A$

\textbf{3$\times$3 Determinant (Sarrus):}
$\begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} = aei + bfg + cdh - ceg - afh - bdi$

\textbf{4$\times$4 (Cofactor Expansion):} Select row with most zeros.

$\det A = a_{11}C_{11} - a_{12}C_{12} + a_{13}C_{13} - a_{14}C_{14}$

\vspace{4pt}
\textbf{\large Rank}

$\text{rank}(A) = $ number of non-zero rows in echelon form

Full rank (rank $= n$) $\Leftrightarrow \det(A) \neq 0$

Rank $< n$ $\Leftrightarrow \det(A) = 0$

\hrulefill

\vspace{4pt}
\textbf{\large Linear Systems (Ax = b)}

Unique solution: $\det(A) \neq 0$ (pivot in every variable column)

No solution: row $[0 \; 0 \; \cdots \; 0 \mid b]$ with $b \neq 0$

Infinite solutions: free variables exist (columns without pivots)

\vspace{4pt}
\textbf{\large Linear Combination}

Express $\mathbf{v}$ as $\mathbf{v} = c_1\mathbf{u}_1 + c_2\mathbf{u}_2 + c_3\mathbf{u}_3$

Set up augmented matrix with basis vectors as columns, target as last column:

$\left[\begin{array}{ccc|c} \uparrow & \uparrow & \uparrow & \uparrow \\ \mathbf{u}_1 & \mathbf{u}_2 & \mathbf{u}_3 & \mathbf{v} \\ \downarrow & \downarrow & \downarrow & \downarrow \end{array}\right] \xrightarrow{\text{RREF}} \left[\begin{array}{ccc|c} 1 & 0 & 0 & c_1 \\ 0 & 1 & 0 & c_2 \\ 0 & 0 & 1 & c_3 \end{array}\right]$

Solution: $\mathbf{v} = c_1\mathbf{u}_1 + c_2\mathbf{u}_2 + c_3\mathbf{u}_3$

\vspace{4pt}
\textbf{\large Cramer's Rule}

For $A\mathbf{x} = \mathbf{b}$: \quad $x_i = \dfrac{\det(A_i)}{\det(A)}$

where $A_i$ = matrix $A$ with column $i$ replaced by $\mathbf{b}$.

\vspace{4pt}
\textbf{Characteristic Polynomial:} $p(\lambda) = \det(A - \lambda I)$

\textbf{Finding Eigenvalues:} Solve $\det(A - \lambda I) = 0$

\textbf{Finding Eigenvectors:} Solve $(A - \lambda I)\mathbf{x} = \mathbf{0}$

\vspace{4pt}
\textbf{\large Cayley-Hamilton Theorem}

Matrix satisfies its char. eqn: $p(A) = 0$ where $p(\lambda) = \det(A - \lambda I)$

\textbf{Use:} Express $A^n$ in terms of $I$ and $A$ using the characteristic equation.

\textbf{Finding $A^{-1}$:} From $A^2 - A - 2I = 0$, isolate $I$: $2I = A^2 - A$, then multiply by $A^{-1}$: $2A^{-1} = A - I$

\vspace{4pt}
\textbf{\large Diagonalization}

\textbf{Matrix Powers:} $A^n = PD^nP^{-1}$ where $P = [\mathbf{v}_1 \mid \mathbf{v}_2 \mid \cdots]$ (eigenvectors)

\textbf{Matrix Exponential:} $e^{At} = Pe^{Dt}P^{-1}$ where $e^{Dt} = \begin{bmatrix} e^{\lambda_1 t} & 0 \\ 0 & e^{\lambda_2 t} \end{bmatrix}$

Steps: (1) Find eigenvalues/eigenvectors, (2) Form $P$, find $P^{-1}$, (3) Compute $Pe^{Dt}P^{-1}$

\textbf{Diagonal Inverse:}
$D^{-1} = \begin{bmatrix} 1/d_1 & 0 & 0 \\ 0 & 1/d_2 & 0 \\ 0 & 0 & 1/d_3 \end{bmatrix}$

\vspace{4pt}
\textbf{\large Gram-Schmidt Orthogonalization}

Given vectors $\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \ldots\}$, produce orthogonal set $\{\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3, \ldots\}$:

$\mathbf{u}_1 = \mathbf{v}_1$ \quad $\mathbf{u}_2 = \mathbf{v}_2 - \dfrac{\mathbf{v}_2 \cdot \mathbf{u}_1}{\|\mathbf{u}_1\|^2}\mathbf{u}_1$

$\mathbf{u}_3 = \mathbf{v}_3 - \dfrac{\mathbf{v}_3 \cdot \mathbf{u}_1}{\|\mathbf{u}_1\|^2}\mathbf{u}_1 - \dfrac{\mathbf{v}_3 \cdot \mathbf{u}_2}{\|\mathbf{u}_2\|^2}\mathbf{u}_2$

\textbf{Projection formula:} $\text{proj}_{\mathbf{u}}\mathbf{v} = \dfrac{\mathbf{v} \cdot \mathbf{u}}{\|\mathbf{u}\|^2}\mathbf{u}$

Note: $\|\mathbf{u}\|^2 = \mathbf{u} \cdot \mathbf{u}$

\vspace{4pt}
\textbf{\large Matrix Exponential $e^{At}$}

\textbf{Fulmer's Method:}

1. Find char. eqn: $|\lambda I - A| = 0$

2. Convert to DE: $\lambda^2 + 2\lambda + 6 \Rightarrow (D^2 + 2D + 6)y = 0$

3. Solve DE: $y = A_1 e^{m_1 t} + A_2 e^{m_2 t}$

4. Replace constants with $\xi$'s: $e^{At} = \xi_1 e^{m_1 t} + \xi_2 e^{m_2 t}$

5. Differentiate both sides up to $(n-1)$th derivative

6. Set $t = 0$ for each equation to create system

7. Solve for $\xi$'s, substitute back

